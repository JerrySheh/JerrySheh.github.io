import{_ as i,c as a,f as e,o as n}from"./app-D22ydJtp.js";const t="/images/hadoop/dfs.png",l="/images/hadoop/dfs_2.png",p={};function d(h,s){return n(),a("div",null,s[0]||(s[0]=[e(`<h1 id="一、-connection-refused" tabindex="-1"><a class="header-anchor" href="#一、-connection-refused"><span>一、 Connection refused</span></a></h1><p>根据官方文档 Hadoop 3.0 配置，在</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>sbin/start-dfs.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>的时候报错，</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>pdsh@ubuntu: localhost: connect: Connection refused</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>原因是pdsh默认采用的是rsh登录，修改成ssh登录即可，在环境变量/etc/profile里加入：</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>export PDSH_RCMD_TYPE=ssh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>然后<code>source /etc/profile</code> 之后重新sbin/start-dfs.sh</p><hr><h1 id="二、jps-没有-namenode-或者-datanode" tabindex="-1"><a class="header-anchor" href="#二、jps-没有-namenode-或者-datanode"><span>二、jps 没有 namenode 或者 datanode</span></a></h1><p>每次开机都得重新格式化一下namenode才可以看到namenode</p><p>格式化了namenode，datanode又没有了</p><p>其实问题就出在tmp文件，默认的tmp文件每次重新开机会被清空，与此同时namenode的格式化信息就会丢失</p><p>于是我们得重新配置一个tmp文件目录</p><p>首先在home目录下建立一个hadoop_tmp目录</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>sudo mkdir ~/hadoop_tmp</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>然后修改hadoop-3.0.0/etc/hadoop目录里面的core-site.xml文件，加入以下节点：</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>&lt;property&gt;</span></span>
<span class="line"><span>    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span></span>
<span class="line"><span>  &lt;value&gt;/home/jerrysheh/hadoop_tmp&lt;/value&gt;</span></span>
<span class="line"><span>    &lt;description&gt;A base for other temporary directories.&lt;/description&gt;</span></span>
<span class="line"><span>&lt;/property&gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>注意：我的用户是jerrysheh，所以目录是/home/jerrysheh/hadoop_tmp</p><p>OK了，重新格式化Namenode</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>hadoop namenode -format</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>然后启动</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>hadoopstart-all.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>然后输入 jps， namenode 和 datanode 应该都出来了</p><hr><h1 id="三、关于-hdfs-文件夹位置" tabindex="-1"><a class="header-anchor" href="#三、关于-hdfs-文件夹位置"><span>三、关于 HDFS 文件夹位置</span></a></h1><p>根据 <a href="https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener noreferrer">官方文档</a></p><p>第一次运行，启动 <code>start-dfs.sh</code> 之后，要先创建用户</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>bin/hdfs dfs -mkdir /user</span></span>
<span class="line"><span>bin/hdfs dfs -mkdir /user/&lt;username&gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>然后创建一个 <code>input</code> 文件夹，用来放数据</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>bin/hdfs dfs -mkdir input</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="`+t+'" alt="dfs"></p><p>发现了吗？ -mkdir /user 和 -mkdir /user/&lt;username&gt; 是在根目录下创建 user 文件夹，然后在 user 文件夹里创建 username 文件夹， 这没有问题。</p><p>但是创建 input 文件夹的时候， 前面没有 /</p><p>意味着，是在默认的 username 文件夹里面创建了这个 input ！</p><p>也就是， input 的实际位置在 /user/&lt;username&gt;/input</p><p>打开 web UI ( 127.0.0.1:8088 )看一下</p><p><img src="'+l+`" alt="dfs"></p><p>果然如此</p><p>另，</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>hdfs dfs -ls .   /*表示当前用户目录*/</span></span>
<span class="line"><span>hdfs dfs -ls /   /*表示根目录*/</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>使用 <code>hdfs dfs -ls .</code> 的时候， HDFS 上的 username 必须和你本地linux系统的 username 一致！否则会显示没有该目录或文件。</li></ul><hr><h1 id="四、windows-环境下-java-home-路径不对" tabindex="-1"><a class="header-anchor" href="#四、windows-环境下-java-home-路径不对"><span>四、Windows 环境下 JAVA_HOME 路径不对</span></a></h1><p>配置好环境以后，执行格式化</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>hdfs namenode -format</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>然后报错</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>Error: JAVA_HOME is incorrectly set.</span></span>
<span class="line"><span>       Please update F:\\hadoop\\conf\\hadoop-env.cmd</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>原因是蛋疼的微软， Program Files文件夹有一个空格，导致不能被 Hadoop 识别。</p><p>解决办法：</p><ul><li><p>方法1：用路径替代符</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>C:\\PROGRA~1\\Java\\jdk1.8.0_91</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><code>PROGRA~1</code> ===== <code>C:\\Program Files</code> 目录的dos文件名模式下的缩写 长于8个字符的文件名和文件夹名，都被简化成前面6个有效字符，后面~1，有重名的就 <sub>2,</sub>3,</p></li><li><p>方法2：用引号括起来</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>&quot;C:\\Program Files&quot;\\Java\\jdk1.8.0_91</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></li></ul><hr><h1 id="五、idea-报-java-lang-classnotfoundexception-问题" tabindex="-1"><a class="header-anchor" href="#五、idea-报-java-lang-classnotfoundexception-问题"><span>五、IDEA 报 java.lang.ClassNotFoundException 问题</span></a></h1><p>在 IDEA 本地环境运行 hadoop 程序时， IDEA报错</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>原因：</p><p>在 maven 中， 把依赖项的 <code>provided</code> 标签删掉即可 。因为加上<code>provided</code>标签意味着 the scope tag tells Maven that you&#39;re using this dependency for building, but it indicates that the dependency will be provided during runtime, so you&#39;ll either need to remove this tag or ... (具体可到 <a href="https://stackoverflow.com/questions/29092926/java-lang-classnotfoundexception-org-apache-hadoop-conf-configuration" target="_blank" rel="noopener noreferrer">StackOverFlow</a> 看)</p><p>感谢 StackOverFlow 解决困扰了我一天一夜的问题</p><div class="language-xml line-numbers-mode" data-ext="xml" data-title="xml"><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">    &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">org.apache.hadoop</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">    &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">hadoop-common</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">    &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">3.0.0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">     &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h1 id="六、idea-hadoop程序-单机运行-和-在本地伪分布式-运行" tabindex="-1"><a class="header-anchor" href="#六、idea-hadoop程序-单机运行-和-在本地伪分布式-运行"><span>六、IDEA Hadoop程序 单机运行 和 在本地伪分布式 运行</span></a></h1><h2 id="单机运行" tabindex="-1"><a class="header-anchor" href="#单机运行"><span>单机运行</span></a></h2><p>新建 Java Maven 工程，<code>pom.xml</code>添加依赖</p><div class="language-xml line-numbers-mode" data-ext="xml" data-title="xml"><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependencies</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">org.apache.hadoop</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">hadoop-common</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">3.0.0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">             &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">org.apache.hadoop</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">hadoop-client</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">3.0.0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">scope</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">provided</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">scope</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">org.apache.hadoop</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">hadoop-mapreduce-client-core</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">3.0.0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">org.apache.hadoop</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">hadoop-mapreduce-client-common</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">3.0.0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">org.apache.hadoop</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">groupId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">hadoop-hdfs</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">artifactId</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">            &lt;</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">3.0.0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">version</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">        &lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependency</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">    &lt;/</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">dependencies</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">&gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p>依赖去哪里找？ 有一个 mavenrepository 仓库，地址是：<a href="https://mvnrepository.com/" target="_blank" rel="noopener noreferrer">mavenrepository</a></p></blockquote><blockquote><p>然后在上面搜索 hadoop ，一些常见的依赖在上面，点击，选择版本，然后复制它的maven代码， 在<code>pom.xml</code>粘贴。 当然这里有坑！ 看上面第五！！</p></blockquote><p>然后右下角 import， maven 会自动帮我们下载依赖包</p><p>点 Run -&gt; Edit configuration， Program argument填入</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>input/ output/</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Main class 填你的main程序所在的 class ，可以输入前几个字母然后 IDEA 会自动帮我们检索</p><p>然后在项目目录下，新建一个文件夹 input ， 往里面放你输入的数据（比如 mydata.log），可以放多个文件</p><p>然后运行即可</p><h2 id="本地伪分布式运行" tabindex="-1"><a class="header-anchor" href="#本地伪分布式运行"><span>本地伪分布式运行</span></a></h2><p>本地Hadoop环境先配起来，具体看这篇</p><p>http://blog.csdn.net/songhaifengshuaige/article/details/79575308</p><p>环境变量</p><p>HADOOP_HOME， 设置为 <code>C:\\hadoop-3.0.0</code> （根据你的目录） Path， 添加<code>%HADOOP_HOME%\\bin</code> 和 <code>%HADOOP_HOME%\\sbin</code></p><p>然后先启动 <code>start-dfs.cmd</code> 和 <code>start-yarn.cmd</code></p><p>输入<code>jps</code>命令，看看 datanode 和 namenode 启动没，确保集群环境启动了</p><p>然后把 <code>C:\\hadoop-3.0.0\\etc\\hadoop\\</code> 里面的配置文件 （几个dfs、core、mapred、yarn相关的 xml 文件），放到 项目 src/main/resource 里面</p><p>然后运行。DONE！</p><hr><h1 id="七、spark-standalone-模式集群配置" tabindex="-1"><a class="header-anchor" href="#七、spark-standalone-模式集群配置"><span>七、Spark standalone 模式集群配置</span></a></h1><p>记得关防火墙</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>service iptables status</span></span>
<span class="line"><span>service iptables stop</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>有一个WARN，提示你本地ip是127.0.0.1，应该该到 172.x.x.x 或 192.x.x.x ，否则局域网机器访问不到。</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>cp ./conf/spark-env.sh.template ./conf/spark-env.sh</span></span>
<span class="line"><span>vim ./conf/spark-env.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>添加</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>SPARK_LOCAL_IP=172.x.x.x</span></span>
<span class="line"><span>SPARK_MASTER_HOST=172.x.x.x</span></span>
<span class="line"><span>SPARK_EXECUTOR__MEMORY=16G</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,89)]))}const o=i(p,[["render",d],["__file","index.html.vue"]]),k=JSON.parse('{"path":"/article/qaa6vtx5/","title":"Hadoop 配置遇到的一些坑","lang":"zh-CN","frontmatter":{"title":"Hadoop 配置遇到的一些坑","comments":true,"categories":["大数据","hadoop"],"tags":["Hadoop"],"abbrlink":"a44bfe3f","createTime":"2018/03/15 22:20:00","permalink":"/article/qaa6vtx5/","description":"一、 Connection refused 根据官方文档 Hadoop 3.0 配置，在 的时候报错， 原因是pdsh默认采用的是rsh登录，修改成ssh登录即可，在环境变量/etc/profile里加入： 然后source /etc/profile 之后重新sbin/start-dfs.sh 二、jps 没有 namenode 或者 datanode...","head":[["meta",{"property":"og:url","content":"https://jerrysheh.com/article/qaa6vtx5/"}],["meta",{"property":"og:site_name","content":"Jerry"}],["meta",{"property":"og:title","content":"Hadoop 配置遇到的一些坑"}],["meta",{"property":"og:description","content":"一、 Connection refused 根据官方文档 Hadoop 3.0 配置，在 的时候报错， 原因是pdsh默认采用的是rsh登录，修改成ssh登录即可，在环境变量/etc/profile里加入： 然后source /etc/profile 之后重新sbin/start-dfs.sh 二、jps 没有 namenode 或者 datanode..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://jerrysheh.com/images/hadoop/dfs.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-10-20T05:02:32.000Z"}],["meta",{"property":"article:tag","content":"Hadoop"}],["meta",{"property":"article:modified_time","content":"2024-10-20T05:02:32.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Hadoop 配置遇到的一些坑\\",\\"image\\":[\\"https://jerrysheh.com/images/hadoop/dfs.png\\",\\"https://jerrysheh.com/images/hadoop/dfs_2.png\\"],\\"dateModified\\":\\"2024-10-20T05:02:32.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":2,"title":"单机运行","slug":"单机运行","link":"#单机运行","children":[]},{"level":2,"title":"本地伪分布式运行","slug":"本地伪分布式运行","link":"#本地伪分布式运行","children":[]}],"readingTime":{"minutes":4.26,"words":1278},"git":{"createdTime":1729400552000,"updatedTime":1729400552000,"contributors":[{"name":"jerrysheh","email":"jerrysheh@gmail.com","commits":1}]},"autoDesc":true,"filePathRelative":"posts/bigdata/hadoop/Hadoop-配置遇到的一些坑.md","categoryList":[{"id":"18958e","sort":10000,"name":"posts"},{"id":"e8c0d3","sort":10001,"name":"bigdata"},{"id":"b13d0a","sort":10017,"name":"hadoop"}]}');export{o as comp,k as data};
