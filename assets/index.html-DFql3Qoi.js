import{_ as a,c as t,f as s,o as i}from"./app-D22ydJtp.js";const r={};function n(o,e){return i(),t("div",null,e[0]||(e[0]=[s(`<p>Spark Streaming 除了可以从三个基本数据源（文件、TCP套接字、RDD）读取数据，还能从高级的数据源中读取。这里的高级数据源，指的是 Kafka 或 Flume。</p><h1 id="kafka-简介" tabindex="-1"><a class="header-anchor" href="#kafka-简介"><span>Kafka 简介</span></a></h1><p>Kafka 是 LinkedIn 开发的一种高吞吐量的分布式发布订阅消息系统。 相比更擅长批量离线处理的 Flume 和 Scribe， Kafka 的优点是可以同时满足在线实时处理和批量离线处理。</p><p>Kafka 在很多公司的大数据平台中，通常扮演数据交换枢纽角色。Hadoop 生态系统中有很多的组件，每当有一种新的产品加入到企业的大数据生态系统中时，就要为这款产品开发与 Hadoop 各个组件的数据交换工具，显得比较麻烦。 而 Kafka，是一种通用工具，起到数据交换枢纽的作用。</p><p>不同的分布式系统（如关系数据库、流处理系统、批处理系统等）都可以统一接入 Kafka。</p><h2 id="kafka的核心概念" tabindex="-1"><a class="header-anchor" href="#kafka的核心概念"><span>Kafka的核心概念</span></a></h2><p><img src="https://kafka.apache.org/11/images/kafka-apis.png" alt="Kafka"></p><ul><li><strong>Broker</strong>： Kafka集群包含一个或多个服务器，这种服务器被称为broker</li><li><strong>Topic</strong>： 每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</li><li><strong>Partition</strong>： Partition是物理上的概念，每个Topic包含一个或多个Partition.</li><li><strong>Producer</strong>： 负责发布消息到Kafka broker</li><li><strong>Consumer</strong>： 消息消费者，向Kafka broker读取消息的客户端。</li><li><strong>Consumer Group</strong>： 每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）</li></ul><h2 id="kafka简单实例" tabindex="-1"><a class="header-anchor" href="#kafka简单实例"><span>Kafka简单实例</span></a></h2><ol><li>安装完 Kafka 后， 会发现解压的 kafka 文件夹不是属于我的用户组， 修改所有者</li></ol><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>sudo chown -R jerrysheh ./kafka</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="2"><li>Kafka 是基于 zookeeper 的，因此首先要开启 zookeeper 服务</li></ol><p>shell 1</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>bin/zookeeper-server-start.sh config/zookeeper.properties</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="3"><li>开启 kafka 服务</li></ol><p>shell 2</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>bin/kafka-server-start.sh config/server.properties</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="4"><li>topic是发布消息发布的 category , 现在以单节点的配置创建一个叫test的topic</li></ol><p>shell 3</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="5"><li>用 <code>--list</code> 参数查看 topic 是否存在</li></ol><p>shell 3</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>bin/kafka-topics.sh --list --zookeeper localhost:2181</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="6"><li>用 producer 生产一些数据</li></ol><p>shell 3</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>随便输入点数据，比如</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>This is a message</span></span>
<span class="line"><span>This is another message</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ol start="7"><li>用consumer来接收数据</li></ol><p>shell 4</p><div class="language- line-numbers-mode" data-ext="" data-title=""><button class="copy" title="复制代码" data-copied="已复制"></button><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>可以看到，刚刚我们用producer生产的数据，在shell 4 ，也就是consumer终端接收到了。</p><p>参考：<a href="https://kafka.apache.org/quickstart" target="_blank" rel="noopener noreferrer">官方文档</a></p><hr><h1 id="使用-kafka-作为-dstream-数据源" tabindex="-1"><a class="header-anchor" href="#使用-kafka-作为-dstream-数据源"><span>使用 Kafka 作为 DStream 数据源</span></a></h1><p>放弃治疗</p>`,36)]))}const p=a(r,[["render",n],["__file","index.html.vue"]]),c=JSON.parse('{"path":"/article/jwvur8xx/","title":"使用 Spark Streaming 进行实时流计算(二)","lang":"zh-CN","frontmatter":{"title":"使用 Spark Streaming 进行实时流计算(二)","comments":false,"categories":["大数据","Spark"],"tags":["大数据"],"abbrlink":76774483,"createTime":"2018/04/07 16:33:49","permalink":"/article/jwvur8xx/","description":"Spark Streaming 除了可以从三个基本数据源（文件、TCP套接字、RDD）读取数据，还能从高级的数据源中读取。这里的高级数据源，指的是 Kafka 或 Flume。 Kafka 简介 Kafka 是 LinkedIn 开发的一种高吞吐量的分布式发布订阅消息系统。 相比更擅长批量离线处理的 Flume 和 Scribe， Kafka 的优点是...","head":[["meta",{"property":"og:url","content":"https://jerrysheh.com/article/jwvur8xx/"}],["meta",{"property":"og:site_name","content":"Jerry"}],["meta",{"property":"og:title","content":"使用 Spark Streaming 进行实时流计算(二)"}],["meta",{"property":"og:description","content":"Spark Streaming 除了可以从三个基本数据源（文件、TCP套接字、RDD）读取数据，还能从高级的数据源中读取。这里的高级数据源，指的是 Kafka 或 Flume。 Kafka 简介 Kafka 是 LinkedIn 开发的一种高吞吐量的分布式发布订阅消息系统。 相比更擅长批量离线处理的 Flume 和 Scribe， Kafka 的优点是..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://kafka.apache.org/11/images/kafka-apis.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-10-20T05:02:32.000Z"}],["meta",{"property":"article:tag","content":"大数据"}],["meta",{"property":"article:modified_time","content":"2024-10-20T05:02:32.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"使用 Spark Streaming 进行实时流计算(二)\\",\\"image\\":[\\"https://kafka.apache.org/11/images/kafka-apis.png\\"],\\"dateModified\\":\\"2024-10-20T05:02:32.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":2,"title":"Kafka的核心概念","slug":"kafka的核心概念","link":"#kafka的核心概念","children":[]},{"level":2,"title":"Kafka简单实例","slug":"kafka简单实例","link":"#kafka简单实例","children":[]}],"readingTime":{"minutes":2.48,"words":744},"git":{"createdTime":1729400552000,"updatedTime":1729400552000,"contributors":[{"name":"jerrysheh","email":"jerrysheh@gmail.com","commits":1}]},"autoDesc":true,"filePathRelative":"posts/bigdata/spark/使用-Spark-Streaming-进行实时流计算（二）.md","categoryList":[{"id":"18958e","sort":10000,"name":"posts"},{"id":"e8c0d3","sort":10001,"name":"bigdata"},{"id":"5133c6","sort":10019,"name":"spark"}]}');export{p as comp,c as data};
