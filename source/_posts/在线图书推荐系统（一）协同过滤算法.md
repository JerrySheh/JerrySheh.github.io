---
title: 在线图书推荐系统（一）协同过滤原理和数据集准备
comments: true
categories: 大数据
tags: 大数据
abbrlink: 6d329abe
date: 2018-05-13 15:24:24
---

最近要做一个简易的在线图书推荐系统，这个项目可能成为之后我的毕业设计。所以将学习的每一步记录下来。

参考教程：

- [Part I: Building the recommender](https://github.com/jadianes/spark-movie-lens/blob/master/notebooks/building-recommender.ipynb)
- [Part II: Building and running the web service](https://github.com/jadianes/spark-movie-lens)
- [厦门大学数据库实验室](http://dblab.xmu.edu.cn/blog/1781-2/)

参考书籍：

- [大数据技术原理与应用 林子雨（人民邮电出版社）](http://dblab.xmu.edu.cn/post/bigdata/)

参考链接：

- [Spark 官方文档- Collaborative Filtering](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html)
- [Github项目 - An on-line movie recommender using Spark, Python Flask, and the MovieLens dataset](https://github.com/jadianes/spark-movie-lens)
- [在线图书推荐系统的实现（协同过滤）](https://www.jianshu.com/p/32d7a2d993a8)
- [Github项目 - spark-book-recommender-system](https://github.com/XuefengHuang/RecommendationSystem)

<!-- more -->

---

# 常用的推荐算法

## 基于人口统计学的推荐(Demographic-Based Recommendation)

该方法所基于的基本假设是“一个用户有可能会喜欢与其相似的用户所喜欢的物品”。当我们需要对一个User进行个性化推荐时，利用User Profile计算其它用户与其之间的相似度，然后挑选出与其最相似的前K个用户，之后利用这些用户的购买和打分信息进行推荐。

## 基于内容的推荐(Content-Based Recommendation)

Content-Based方法所基于的基本假设是“一个用户可能会喜欢和他曾经喜欢过的物品相似的物品”。

## 基于协同过滤的推荐（Collaborative Filtering Recommendation）

协同过滤是指收集用户过去的行为以获得其对产品的显式或隐式信息，即根据用户对物品或者信息的偏好，**发现物品或者内容本身的相关性**、或用户的相关性，然后再基于这些关联性进行推荐。

基于协同过滤的推荐可以分为以下几个子类：

- **基于用户的推荐（User-based Recommendation）**
- **基于物品的推荐（Item-based Recommendation）**
- **基于模型的推荐（Model-based Recommendation）**

协同过滤的本质是对用户喜好进行预测，其思想是根据邻居用户(与目标用户兴趣相似的用户)的偏好信息，计算出某用户对某商品的感兴趣度。

协同过滤的基本假设（underlying assumption）是，用户A 和 用户B 如果对某些商品持有相同的观点，那么 用户A 跟 用户B 对 另一些商品 的观点会更接近。

![example](https://camo.githubusercontent.com/a6e062883b83adb3b65b5a9e167a3a6f5e5f9a19/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f352f35322f436f6c6c61626f7261746976655f66696c746572696e672e676966)

如果我们用一个用户-产品矩阵来描述用户对产品的喜好程度，那么协同过滤算法的目标就是填充缺失的元素，如下图所示。（These techniques aim to fill in the missing entries of a user-item association matrix）

![fill](../../../../images/hadoop/cf.png)

---
# Spark.ml库中的推荐算法

spark.ml 库支持基于模型的协同过滤推荐推荐算法，其中用户和商品通过一小组隐语义因子(latent factors)进行表达，并且这些因子也用于预测缺失的元素。spark.ml库使用 ALS算法 来学习这些隐性语义因子。

## ALS

`交替最小二乘法 (Alternating Least Squares，ALS)`常用于基于矩阵分解的推荐系统中。

例如：将用户(user)对商品(item)的评分矩阵分解为两个矩阵：

- 一个是用户对商品隐含特征的偏好矩阵
- 另一个是商品所包含的隐含特征的矩阵。

在这个矩阵分解的过程中，评分缺失项得到了填充，也就是说我们可以基于这个填充的评分来给用户最商品推荐了。

## 隐性反馈（implicit feedback）和显性反馈（explicit feedback）

基于矩阵分解的协同过滤的标准方法一般将用户商品矩阵中的元素作为用户对商品的显性偏好。 在许多的现实生活中的很多场景中，我们常常只能接触到隐性的反馈（例如游览，点击，购买，喜欢，分享等等）

在 MLlib 中所用到的处理这种数据的方法来源于文献： [Collaborative Filtering for Implicit Feedback Datasets](http://dx.doi.org/10.1109/ICDM.2008.22)。 本质上，这个方法将数据作为二元偏好值和偏好强度的一个结合，而不是对评分矩阵直接进行建模。因此，评价就不是与用户对商品的显性评分而是和所观察到的用户偏好强度关联了起来。然后，这个模型将尝试找到隐语义因子来预估一个用户对一个商品的偏好。

---

# 数据集准备

这里使用 [Book-Crossing](http://www2.informatik.uni-freiburg.de/~cziegler/BX/) 的数据集进行学习。

该数据集包含三个部分：

- 用户数据 BX-User.csv

0|1|2|
---|---|---
 "User-ID"|"Location"|"Age"

```
"1";"nyc, new york, usa";NULL
"2";"stockton, california, usa";"18"
"3";"moscow, yukon territory, russia";NULL
"4";"porto, v.n.gaia, portugal";"17"
```



- 图书数据 BX-Books.csv

0|1|2|3|4|5|6|7
 ---|---|---|---|---|---|---|---
 "ISBN"|"Book-Title"|"Book-Author"|"Year-Of-Publication"|"Publisher"|"Image-URL-S"|"Image-URL-M"|"Image-URL-L"

```
"0195153448";"Classical Mythology";"Mark P. O. Morford";"2002";"Oxford University Press";"http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg";"http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg";"http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg"
"0002005018";"Clara Callan";"Richard Bruce Wright";"2001";"HarperFlamingo Canada";"http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg";"http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg";"http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg"
```



- 用户对图书的评分数据 BX-Book-Ratings.csv

0|1|2|
---|---|---
"User-ID"|"ISBN"|"Book-Rating"

```
"276725";"034545104X";"0"
"276726";"0155061224";"5"
"276727";"0446520802";"0"
"276729";"052165615X";"3"
"276729";"0521795028";"6"
```

---

# 数据集清洗

## 用 Python 提取关键信息

可以用 Python 的 `split()` 方法，对数据集信息进行分割以备装入 RDDs

需要对 图书数据集 和 评分数据集 进行处理：

- 对于评分数据集，创建一个 tuple 包含 (UserID, BookID, Rating)
- 对于图书数据集，创建一个 tuple 包含 (ISBN，Book-Title，Book-Author，Year-Of-Publication，Publisher，Image-URL-L)


评分数据

```python
# Load ratings data for later use
logger.info("Loading Ratings data...")
ratings_file_path = os.path.join(dataset_path, 'BX-Book-Ratings.csv')
ratings_raw_RDD = self.sc.textFile(ratings_file_path)
ratings_raw_data_header = ratings_raw_RDD.take(1)[0]
self.ratings_RDD = ratings_raw_RDD.filter(lambda line: line!=ratings_raw_data_header)\
    .map(lambda line: line.split(";"))\
    .map(lambda tokens: (int(tokens[0][1:-1]), abs(hash(tokens[1][1:-1])) % (10 ** 8), int(tokens[2][1:-1]))).cache()
```

图书数据

```python
# Load books data for later use
logger.info("Loading Books data...")
books_file_path = os.path.join(dataset_path, 'BX-Books.csv')
books_raw_RDD = self.sc.textFile(books_file_path)
books_raw_data_header = books_raw_RDD.take(1)[0]
self.books_RDD = books_raw_RDD.filter(lambda line: line!=books_raw_data_header)\
    .map(lambda line: line.split(";"))\
    .map(lambda tokens: (abs(hash(tokens[0][1:-1])) % (10 ** 8), tokens[1][1:-1], tokens[2][1:-1], tokens[3][1:-1], tokens[4][1:-1], tokens[5][1:-1])).cache()
self.books_titles_RDD = self.books_RDD.map(lambda x: (int(x[0]), x[1], x[2], x[3], x[4], x[5])).cache()
```

未完待续
